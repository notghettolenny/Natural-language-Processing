{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "0.23.3"
    },
    "colab": {
      "name": "Shakespeare Sonnet Generator, poems using Recurrent Neural Networks and LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQxcSg4o1sRN"
      },
      "source": [
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "Our goal in this projectis to build a **Shakespeare Sonnet Generator**.<br>\n",
        "Given a prompt of a few words as input, its task is to generate follow-on text that reads like a Shakespeare Sonnet!<br>\n",
        "\n",
        "To build our Sonnet Generator we will use a type of model called a **sequence model**. Given a short sequence, a sequence  model predicts the **most likely next item in the sequence**. Sequence models are astonishingly versatile and powerful, because the **sequence** we want to predict can be quite general! It can be composed of **words**, or of **characters**, or of **musical notes**, or of data points in a **time series** such as EKG voltages, or stock prices, or even a sequence of **DNA nucleotides**! \n",
        "\n",
        "We will train our model on the entire corpus of Shakespeare's Sonnets, and the model will learn from that data the most likely patterns of characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-06-15T18:18:20.453Z",
          "iopub.status.busy": "2020-06-15T18:18:20.442Z",
          "iopub.status.idle": "2020-06-15T18:18:20.513Z",
          "shell.execute_reply": "2020-06-15T18:18:20.523Z"
        },
        "id": "L-AX4IBIcqLK"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# import a custom text data preparation class\n",
        "!wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/main/module1-rnn-and-lstm/data_cleaning_toolkit_class.py\n",
        "from data_cleaning_toolkit_class import data_cleaning_toolkit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db6enoAacqLL"
      },
      "source": [
        "### Use `request` to pull data from a URL\n",
        "\n",
        "[**Read through the request documentation**](https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request) to learn how to download the Shakespeare Sonnets from the Gutenberg website. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6ac79c2e9a53d747ebf8fb41f4b39340",
          "grade": false,
          "grade_id": "cell-b8ececfad1f60557",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "nMf2XrJbcqLM"
      },
      "source": [
        "# download all of Shakespeare's Sonnets from the Project Gutenberg website\n",
        "\n",
        "# here's the link for the sonnets\n",
        "url_shakespeare_sonnets = \"https://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
        "\n",
        "# use requests and the url to download all of the sonnets - save the result to `data`\n",
        "data = requests.get(url_shakespeare_sonnets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.text"
      ],
      "metadata": {
        "id": "Fnav_C8SfpTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4ab4f4f14188a9f3703d43d223bfa150",
          "grade": false,
          "grade_id": "cell-0cd0c8509bc8e8cf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dcXTQ5RTcqLM"
      },
      "source": [
        "# extract the downloaded text from the requests object and save it to `raw_text_data`\n",
        "# hint: take a look at the attributes of `data`\n",
        "# YOUR CODE HERE\n",
        "raw_text_data = data.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW4mj8eScqLN"
      },
      "source": [
        "# check the data type of `raw_text_data`\n",
        "assert(type(raw_text_data)==str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwAX_OwEcqLN"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j7G1zqncqLO"
      },
      "source": [
        "# as usual, we need to clean up the messy data\n",
        "raw_text_data[:3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "13b66e41cc64459f0757f6f53a78e08f",
          "grade": false,
          "grade_id": "cell-916f742d2cea299a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JolH-nrVcqLP"
      },
      "source": [
        "# Which characters could we use with the split() method to split the text into lines?\n",
        "\n",
        "# split the text into **lines** and save the result to `split_data`\n",
        "\n",
        "# YOUR CODE HERE\n",
        "split_data = raw_text_data.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_data"
      ],
      "metadata": {
        "id": "ytmZZ5ZQlFtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGAL77acqLQ"
      },
      "source": [
        "# we need to drop all the boiler plate text (i.e. titles and descriptions) as well as extra white spaces\n",
        "# so that we are left with only the sonnets themselves \n",
        "split_data[:80] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fsYTC76cqLQ"
      },
      "source": [
        "**Use list index slicing to remove the titles and descriptions, so we only have the sonnets.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "00ead0a1024ff72116c24f6b473c1aac",
          "grade": false,
          "grade_id": "cell-1f388b88b0eec24a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NLaFgX08cqLR"
      },
      "source": [
        "# we need to drop all the boilerplate text (i.e., titles and descriptions) as well as extra white spaces\n",
        "# so that we are left with only the sonnets themselves \n",
        "\n",
        "# find index boundaries (start, end)so that \n",
        "# sonnets exist between these indices \n",
        "# titles and descriptions exist outside of these indices\n",
        "\n",
        "# use index slicing to isolate the sonnet lines from the text - save the result to `sonnets`\n",
        "\n",
        "# YOUR CODE HERE\n",
        "start = split_data.index('  From fairest creatures we desire increase,\\r')\n",
        "sonnets = split_data[start:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mJReim_43Ma"
      },
      "source": [
        "Notice that there are many lines that should not be counted as sonnets!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th9BuvVlcqLR"
      },
      "source": [
        "# these non-sonnet lines have far fewer characters than the actual sonnet lines?\n",
        "\n",
        "sonnets[200:240]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "649cf52260448a5faf539ad6b6e8e6e8",
          "grade": false,
          "grade_id": "cell-84c4b3cf1f3c032a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "OiOfyPexcqLS"
      },
      "source": [
        "# use your judgement to decide on a good value for  \n",
        "#   the  minimum number of characters that a sonnet should have\n",
        "#   call it n_chars\n",
        "n_chars = 20\n",
        "\n",
        "# Let's use that observation to filter out all the non-sonnet lines!\n",
        "#    save results to `filtered_sonnets`\n",
        "# Hint: use a list comprehension\n",
        "filtered_sonnets = [x.strip()[:-2] for x in sonnets if len(x) > 20]\n",
        "\n",
        "# YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW_AFhCUcqLS"
      },
      "source": [
        "# ok - much better!\n",
        "# but we still need to remove all the punctuation and case normalize the text\n",
        "import re\n",
        "filtered_sonnets = [re.sub(r'[^a-z]', ' ', x.lower()) for x in filtered_sonnets]\n",
        "filtered_sonnets = [re.sub(r'[ ]{2,}', ' ', x.lower()) for x in filtered_sonnets]\n",
        "filtered_sonnets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iD4oqhVcqLT"
      },
      "source": [
        "### Use Custom Data Cleaning Tool \n",
        "\n",
        "Use one of the methods in the `data_cleaning_toolkit` to clean your data.\n",
        "\n",
        "There is an example of this in the guided project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a722083a29139936744ff9a341e1c9a3",
          "grade": false,
          "grade_id": "cell-775c14b456d8a724",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "S7V1Q0h_cqLT"
      },
      "source": [
        "# instantiate the data_cleaning_toolkit class - save result to `dctk`\n",
        "\n",
        "# YOUR CODE HERE\n",
        "dctk = data_cleaning_toolkit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ab91e612cd08068f3a36172979157d5d",
          "grade": false,
          "grade_id": "cell-684010b6a7360876",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "YquHh21GcqLT"
      },
      "source": [
        "# use data_cleaning_toolkit to remove punctuation and to case normalize - save results to `clean_sonnets`\n",
        "\n",
        "# YOUR CODE HERE\n",
        "clean_sonnets = [dctk.clean_data(x) for x in filtered_sonnets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3vWjofcqLT"
      },
      "source": [
        "# much better!\n",
        "display(clean_sonnets)\n",
        "print(len(clean_sonnets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSfirtmYcqLT"
      },
      "source": [
        "### Create Character Sequences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1deebea2ada0a7dc7d2eb08295ee1e2b",
          "grade": false,
          "grade_id": "cell-9ebdaa2654dd29ab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "O7Buzpw3cqLU"
      },
      "source": [
        "def calc_stats(corpus):\n",
        "    \"\"\"\n",
        "    Calculates statistics on the length of every line in the sonnets\n",
        "    \"\"\"\n",
        "    \n",
        "    # write a list comprehension that calculates each sonnet's line length - save the results to `doc_lens` \n",
        "\n",
        "    # use numpy to calculate and return the mean, median, std, max, min of the doc lens - all in one line of code\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    doc_lens = [len(x) for x in clean_sonnets]\n",
        "    return np.mean(doc_lens), np.median(doc_lens), np.std(doc_lens), np.max(doc_lens), np.min(doc_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DGCJO_QcqLU"
      },
      "source": [
        "# sonnet line length statistics \n",
        "mean, med, std, max_, min_ = calc_stats(clean_sonnets)\n",
        "mean, med, std, max_, min_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "690957e46b6f2f32c1f17756d8ceab5b",
          "grade": false,
          "grade_id": "cell-35185e26897aad7e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "WOCylsbLcqLU"
      },
      "source": [
        "# from the results of the sonnet line length statistics, use your judgement to select a value for maxlen\n",
        "#   hint -- a good value might be half the median length of a sonnet line\n",
        "# use .create_char_sequences() to create sequences\n",
        "\n",
        "# YOUR CODE HERE\n",
        "dctk.create_char_sequences(clean_sonnets, 21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8BNLJuhcqLV"
      },
      "source": [
        "# number of input features for our LSTM model\n",
        "dctk.n_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrPRQy3cqLV"
      },
      "source": [
        "# unique characters that appear in our sonnets \n",
        "dctk.unique_chars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAwB6dLHcqLW"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXZMkLckcqLW"
      },
      "source": [
        "### Use Our Data Tool to Create X and Y Splits\n",
        "\n",
        "You'll need the `create_X_and_Y` method for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK-1GQzncqLW"
      },
      "source": [
        "# TODO: provide a walkthrough of data_cleaning_toolkit with unit tests that check for understanding \n",
        "X, y = dctk.create_X_and_Y()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gvcygSTcqLW"
      },
      "source": [
        "![](https://miro.medium.com/max/891/0*jGB1CGQ9HdeUwlgB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xtDCd8wcqLW"
      },
      "source": [
        "# notice that our input array isn't a matrix - it's a rank three tensor\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0_YHHQdcqLX"
      },
      "source": [
        "In $X$.shape, we see three numbers (*n1*, *n2*, *n3*). What do these numbers mean?\n",
        "\n",
        "Well, *n1* tells us the number of samples that we have. But what about the other two?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB-n6_P4cqLX"
      },
      "source": [
        "# first index returns a signle sample, which we can see is a sequence \n",
        "first_sample_index = 0 \n",
        "X[first_sample_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A75Caji7cqLX"
      },
      "source": [
        "Notice that each sequence (i.e., $X[i]$ where $i$ is some index value) is `maxlen` long and <br>\n",
        "has a number of features equal to `dctk.n_features`. <br>Let's try to understand this shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1E8AFAIcqLX"
      },
      "source": [
        "# each sequence is maxlen long and has dctk.n_features number of features\n",
        "X[first_sample_index].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iChccLKWcqLX"
      },
      "source": [
        "**Each row corresponds to a character vector,** and there is `maxlen` number of character vectors. \n",
        "\n",
        "**Each column corresponds to a unique character,** and there are `dctk.n_features` number of features. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbWKVpd5cqLX"
      },
      "source": [
        "# let's index for a single character vector \n",
        "first_char_vect_index = 0\n",
        "X[first_sample_index][first_char_vect_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cVGE3wtcqLX"
      },
      "source": [
        "Notice that there is a single `True` value, and all the rest of the values are `False`. \n",
        "\n",
        "This is a one-hot encoding for which character appears at each index within a sequence. Specifically, the cell above is looking at the first character in the sequence.\n",
        "\n",
        "Only a single character can appear as the first character in a sequence, so there will be a single `True` value, and the rest will be `False`. \n",
        "\n",
        "Let's say that `True` appears in the $ith$ index; by  $ith$ index we mean some index in the general case. So how can we find out which character corresponds to?\n",
        "\n",
        "To answer this question, we need to use the character-to-integer look-up dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ8dIATxcqLX"
      },
      "source": [
        "# take a look at the index to character dictionary\n",
        "# if a TRUE appears in the 0th index of a character vector,\n",
        "# then we know that whatever char you see below next to the 0th key \n",
        "# is the character that character vector is endcoding for\n",
        "dctk.int_char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXy52FuHcqLY"
      },
      "source": [
        "# let's look at an example to tie it all together\n",
        "\n",
        "seq_len_counter = 0\n",
        "\n",
        "# index for a single sample \n",
        "for seq_of_char_vects in X[first_sample_index]:\n",
        "    \n",
        "    # get index with max value, which will be the one TRUE value \n",
        "    index_with_TRUE_val = np.argmax(seq_of_char_vects)\n",
        "    \n",
        "    print (dctk.int_char[index_with_TRUE_val])\n",
        "    \n",
        "    seq_len_counter+=1\n",
        "    \n",
        "print (\"Sequence length: {}\".format(seq_len_counter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-H9jSq6cqLY"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Fo7tn5cqLY"
      },
      "source": [
        "### Build a Shakespeare Sonnet Text Generation Model\n",
        "\n",
        "Now that we have prepped our data (and understood that process), let's finally build out our character generation model, similar to what we did in the guided project.<br>\n",
        "\n",
        "First, we'll create a callback to monitor the training -- by printing a sample of text generated by the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to generate a sample character:"
      ],
      "metadata": {
        "id": "kVc4tb1OSMMr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xgk-JomzwGX"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Helper function to generate a sample character\n",
        "    Input is a predictions vector from our model, for example a set of 27 character probabilities\n",
        "    Output is the index of the generated character \n",
        "    \"\"\"\n",
        "    # convert predictions to an array \n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "\n",
        "    # use the temperature hyper-parameter to \"warp\" (sharpen or spread out) the probability distribution \n",
        "    preds = np.log(preds) / temperature\n",
        "\n",
        "    # use the softmax activation function to create a new list of probabilities \n",
        "    #   corresponding to the \"warped\" probability distribution\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "    # Draw a single sample from a multinomial distribution, given these probabilities\n",
        "    #   The sample will be a one-hot encoded character\n",
        "    \"\"\" Notes on the np.random.multinomial() function \n",
        "       The first argument is the number of \"trials\" we want: 1 in this case\n",
        "       The second argument is the list of probabilities for each character\n",
        "       The third argument is number of sets of \"trials\" we want: again, 1 in this case\n",
        "       By analogy with a dice-rolling experiment: \n",
        "          This \"trial\" consists of generating a single \"throw\" of a die with 27 faces;\n",
        "             each face corresponds to a character and its associated probability\n",
        "    \"\"\"\n",
        "\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    \n",
        "    # return the index that corresponds to the max probability \n",
        "    return np.argmax(probas)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPhu860ZEd3g"
      },
      "source": [
        "Create the `on_epoch_end` function to be passed into `LambdaCallback()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9cBsweJcqLY"
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    \"\"\"\"\n",
        "    Function invoked at the end of each epoch. Prints the text generated by our model.\n",
        "    \"\"\"\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "\n",
        "    # randomly pick a starting index \n",
        "    # will be used to take a random sequence of chars from `text`\n",
        "    start_index = random.randint(0, len(text) - dctk.maxlen - 1)\n",
        "    \n",
        "    # this is our seed string (i.e. input seqeunece into the model)\n",
        "    generated = ''\n",
        "\n",
        "    # start the sentence at index `start_index` and include the next` dctk.maxlen` number of chars\n",
        "    sentence = text[start_index: start_index + dctk.maxlen]\n",
        "\n",
        "    # add to generated\n",
        "    generated += sentence\n",
        "\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    # use model to predict what the next maxlen chars should be that follow the seed string\n",
        "    for i in range(maxlen):\n",
        "\n",
        "        # shape of a single sample in a rank 3 tensor \n",
        "        x_dims = (1, dctk.maxlen, dctk.n_features)\n",
        "        # create an array of zeros with shape x_dims\n",
        "        # recall that python considers zeros and boolean FALSE as the same\n",
        "        x_pred = np.zeros(x_dims)\n",
        "\n",
        "        # create a seq vector for our randomly select sequence \n",
        "        # i.e. create a numerical encoding for each char in the sequence \n",
        "        for t, char in enumerate(sentence):\n",
        "            # for sample 0 in seq index t and character `char` encode a 1 (which is the same as a TRUE)\n",
        "            x_pred[0, t, dctk.char_int[char]] = 1\n",
        "\n",
        "        # next, take the seq vector and pass into model to get a prediction of what the next char should be \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        # use the sample helper function to get index for next char \n",
        "        next_index = sample(preds)\n",
        "        # use look up dict to get next char \n",
        "        next_char = dctk.int_char[next_index]\n",
        "\n",
        "        # append next char to sequence \n",
        "        sentence = sentence[1:] + next_char \n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6xJijWAcqLZ"
      },
      "source": [
        "# need this for on_epoch_end()\n",
        "text = \" \".join(clean_sonnets)\n",
        "print(f'All of Shakespeare\\'s sonnets comprise about {len(text)} characters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the callback object"
      ],
      "metadata": {
        "id": "91FywCwoUzn0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKDdNLhBcqLZ"
      },
      "source": [
        "# create callback object that will print out text generation at the end of each epoch \n",
        "# use for real-time monitoring of model performance\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cq75YmTcqLZ"
      },
      "source": [
        "----\n",
        "### Build and Train Model\n",
        "\n",
        "Build a text generation model using LSTMs. Feel free to reference the model used in the guided project. \n",
        "\n",
        "It is recommended that you train this model to at least 50 epochs (but more if you're computer can handle it). \n",
        "\n",
        "You are free to change up the architecture as you wish. \n",
        "\n",
        "Just in case you have difficultly training a model, there is a pre-trained model saved to a file called `trained_text_gen_model.h5` that you can load (in the same way that you learned how to load in Keras models in Sprint 2 Module 4). "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "7pBpC-Ddwc8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e17312b57e17284124ce562dff81b00d",
          "grade": false,
          "grade_id": "cell-f34be90367fd9071",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3JLmhL36cqLZ"
      },
      "source": [
        "# build text generation model layer by layer \n",
        "# fit model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "%%time\n",
        "### BEGIN SOLUTION\n",
        "## Takes about 4.5 minutes to train the entire corpus for 50 epochs on a colab GPU (1 LSTM layer, 128 neurons)\n",
        "\n",
        "# build a 1 layer LSTM language model \n",
        "maxlen = 80\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "opt = Adam(learning_rate=0.001)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# hidden layer 1 \n",
        "model.add(LSTM(128, \n",
        "               input_shape=(dctk.maxlen, dctk.n_features), # input_shape is (20,27)\n",
        "               return_sequences=False)) # set to true whenever using 2 or more LSTM layers \n",
        "\n",
        "# this is our output layer\n",
        "# recall that n_features = number of characters in the dictionary = 27\n",
        "model.add(Dense(dctk.n_features, \n",
        "                activation='softmax'))\n",
        "\n",
        "# notice that we are using categorical_crossentropy this time around - why?\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=opt)\n",
        "\n",
        "# fit the model\n",
        "# X and y are pretty large, consider sub-sampling\n",
        "model_result = model.fit(X, y,\n",
        "          batch_size=128,\n",
        "          epochs=100,\n",
        "          callbacks=[print_callback])\n",
        "### END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94I0bGvfJQ0a"
      },
      "source": [
        "### Save the trained model to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4amWje1ncqLZ"
      },
      "source": [
        "# save trained model to file \n",
        "model.save(\"trained_text_gen_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWWRV1VJcqLa"
      },
      "source": [
        "### Let's Play With Our Trained Model \n",
        "\n",
        "Now that we have a trained model that, though far from perfect, can generate actual English words, we can look at the predictions to continue learning more about how a text generation model works.\n",
        "\n",
        "We can also take this as an opportunity to unpack the `def on_epoch_end` function to understand better how it works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gktQ5JQqcqLa"
      },
      "source": [
        "# this is our joined clean sonnet data\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSGHFS4QcqLa"
      },
      "source": [
        "# randomly pick a starting index \n",
        "# will be used to take a random sequence of chars from `text`\n",
        "# run this cell a few times and you'll see `start_index` is random\n",
        "start_index = random.randint(0, len(text) - dctk.maxlen - 1)\n",
        "start_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ak7epOKcqLa"
      },
      "source": [
        "# next use the randomly selected starting index to sample a sequence from the `text`\n",
        "\n",
        "# this is our seed string (i.e., input sequence into the model)\n",
        "generated = ''\n",
        "\n",
        "# start the sentence at index `start_index` and include the next` dctk.maxlen` number of chars\n",
        "sentence = text[start_index: start_index + dctk.maxlen]\n",
        "\n",
        "# add to generated\n",
        "generated += sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfbUcUBXcqLa"
      },
      "source": [
        "# display the \"seed string\" i.e. the input sequence into the model\n",
        "print('----- Input seed: \"' + sentence + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nop-G7CcqLa"
      },
      "source": [
        "# use model to predict what the next maxlen chars should be that follow the seed string\n",
        "for i in range(maxlen):\n",
        "\n",
        "    # shape of a single sample in a rank 3 tensor \n",
        "    x_dims = (1, dctk.maxlen, dctk.n_features)\n",
        "    # create an array of zeros with shape x_dims\n",
        "    # recall that python considers zeros and boolean FALSE as the same\n",
        "    x_pred = np.zeros(x_dims)\n",
        "\n",
        "    # create a seq vector for our randomly select sequence \n",
        "    # i.e. create a numerical encoding for each char in the sequence \n",
        "    for t, char in enumerate(sentence):\n",
        "        # for sample 0 in seq index t and character `char` encode a 1 (which is the same as a TRUE)\n",
        "        x_pred[0, t, dctk.char_int[char]] = 1\n",
        "\n",
        "    # next, take the seq vector and pass into model to get a prediction of what the next char should be \n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    # use the sample helper function to get index for next char \n",
        "    next_index = sample(preds)\n",
        "    # use look up dict to get next char \n",
        "    next_char = dctk.int_char[next_index]\n",
        "\n",
        "    # append next char to sequence \n",
        "    sentence = sentence[1:] + next_char "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX9daz2rcqLb"
      },
      "source": [
        "# this is the seed string\n",
        "generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3jnYVwccqLb"
      },
      "source": [
        "# these are the maxlen chars that the model thinks should come after the seed stirng\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1aqFH4BcqLb"
      },
      "source": [
        "# how put it all together\n",
        "generated + sentence"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}