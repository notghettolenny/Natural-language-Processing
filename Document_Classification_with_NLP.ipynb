{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Document Classification with NLP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "bhHywvWWg1gZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfAUasgKoKVc"
      },
      "source": [
        "We are going to run increasingly sophisticated classification models on whisky reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v3SlXgD5Tk9"
      },
      "source": [
        "##Classifier based on TfIdf vectorization of reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "DpfX8zyJGfBs"
      },
      "source": [
        "### Follow Along \n",
        "\n",
        "1. Join the Kaggle Competition https://www.kaggle.com/c/whiskey-201911/submissions\n",
        "2. Download the data\n",
        "3. Train and hyperparameter tune a model using an sklearn pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWCleLr4lyP7"
      },
      "source": [
        "#### Get spacy and restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysPkqrmcl2I8"
      },
      "source": [
        "#YOUR CODE HERE\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNImGQB1l56U"
      },
      "source": [
        "#### import necessary packages, load spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4KcM3TcMO-N"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsNq7a5WP_Tw"
      },
      "source": [
        "Load `spacy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1V3ApAvMDYx"
      },
      "source": [
        "#YOUR CODE HERE\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UimXwG9tGfBs"
      },
      "source": [
        "#### Load Kaggle Whisky Competition Data\n",
        "The goal is to predict the rating from the review text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf-b1D-wGfBt"
      },
      "source": [
        "# !!!!! You may need to change the path !!!!!\n",
        "# You can download these datasets from the Kaggle in-class \n",
        "# competition for your cohort. \n",
        " \n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw--87X5266R"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnMkWvxdGfBu"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMl8ERBiBCHA"
      },
      "source": [
        "train['description'] = train['description'].astype(str)\n",
        "train['category'] = train['category'].astype(str)\n",
        "test['description'] = test['description'].astype(str)\n",
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC6DgRAljptC"
      },
      "source": [
        "train['description'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uPjYRcHMa7N"
      },
      "source": [
        "### 1.1 Clean Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YTBavgJMj3Y"
      },
      "source": [
        "def clean_doc(text):\n",
        "  # COMPLETE THE CODE IN THIS CELL\n",
        "  # remove new line characters\n",
        "  text = text.lower()\n",
        "  text = text.replace('\\\\n', ' ')\n",
        "  # remove numbers from the text\n",
        "  pattern = re.compile('\\xa0')\n",
        "  text = re.sub(pattern, '', text)\n",
        "  text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "  # remove multiple white spaces\n",
        "  text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
        "\n",
        "  # case normalize and strip extra white spaces on the far left and right hand side\n",
        "  text = text.lstrip().rstrip()\n",
        "  return text\n",
        "\n",
        "train['description'] = train['description'].apply(clean_doc)\n",
        "test['description'] = test['description'].apply(clean_doc)\n",
        "train['description'][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWRA6gDHnOaB"
      },
      "source": [
        "train['description'] = train['description'].apply(lambda x: ' '.join([token.lemma_.strip() for token in nlp(x) if \\\n",
        "                                                             ((not token.is_stop) and (not token.is_punct) and \\\n",
        "                                                              (len(token.lemma_.strip()) > 1) and (token.is_alpha))]))\n",
        "test['description'] = test['description'].apply(lambda x: ' '.join([token.lemma_.strip() for token in nlp(x) if \\\n",
        "                                                             ((not token.is_stop) and (not token.is_punct) and \\\n",
        "                                                              (len(token.lemma_.strip()) > 1) and (token.is_alpha))]))\n",
        "\n",
        "train['description'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7BskE81BKOv"
      },
      "source": [
        "from collections import Counter\n",
        "def count(token_lists):\n",
        "    \"\"\"\n",
        "    Calculates some basic statistics about tokens in our corpus (i.e. corpus means collections text data)\n",
        "    \"\"\"\n",
        "    # stores the count of each token\n",
        "    word_counts = Counter()\n",
        "    \n",
        "    # stores the number of docs that each token appears in \n",
        "    appears_in_docs = Counter()\n",
        "\n",
        "    total_docs = len(token_lists)\n",
        "\n",
        "    for token_list in token_lists:\n",
        "        # stores count of every appearance of a token \n",
        "        word_counts.update(token_list)\n",
        "        \n",
        "        # use set() in order to not count duplicates, thereby count the num of docs that each token appears in\n",
        "        appears_in_docs.update(set(token_list))\n",
        "\n",
        "    # build word count dataframe\n",
        "    word_count_dict = zip(word_counts.keys(), word_counts.values())\n",
        "    wc = pd.DataFrame(word_count_dict, columns = ['word', 'count'])\n",
        "\n",
        "    # rank the the word counts\n",
        "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "    total = wc['count'].sum()\n",
        "\n",
        "    # calculate the percent total of each token\n",
        "    wc['fraction_of_total'] = wc['count'].apply(lambda token_count: token_count / total)\n",
        "\n",
        "    # calculate the cumulative percent total of word counts \n",
        "    wc = wc.sort_values(by='rank')\n",
        "    wc['cumulative_fraction_of_total'] = wc['fraction_of_total'].cumsum()\n",
        "\n",
        "    # create dataframe for document stats\n",
        "    t2 = zip(appears_in_docs.keys(), appears_in_docs.values())\n",
        "    ac = pd.DataFrame(t2, columns=['word', 'appears_in_docs'])\n",
        "    \n",
        "    # merge word count stats with doc stats\n",
        "    wc = ac.merge(wc, on='word')\n",
        "\n",
        "    wc['appears_in_fraction_of_docs'] = wc['appears_in_docs'].apply(lambda x: x / total_docs)\n",
        "\n",
        "    return wc.sort_values(by='rank')\n",
        "#token_lists = [doc.split(' ') for doc in train['description']]\n",
        "#count(token_lists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdqYMpmFMvto"
      },
      "source": [
        "### Split training data into Feature Matrix `X` and Target Vector `y`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJFIJlu3MzH7"
      },
      "source": [
        "target = 'category'\n",
        "# COMPLETE THE CODE IN THIS CELL\n",
        "y = train[target]\n",
        "X = train['description']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IHzPRZ2GfBv"
      },
      "source": [
        "### Specify the Model and Define the Pipeline Components\n",
        "\n",
        "For the classifier model, you can try any or several of \n",
        "* `RandomForestClassifier()` or `GradientBoostingClassifier()` from the `sklearn` library\n",
        "* `XGBClassifier()` from the `xgboost` library\n",
        "* `CatboostClassifier()` from the `catboost` library\n",
        "* `LGBMClassifier()` from the `lightgbm` library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju6TgtdzGfBw"
      },
      "source": [
        "# limit max_features to 500 to speed up training on Colab.\n",
        "# COMPLETE THE CODE IN THIS CELL\n",
        "vect = TfidfVectorizer(stop_words=\"english\")\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "pipe = Pipeline([('vect', vect), ('clf', clf)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiIoAlkaEOCz"
      },
      "source": [
        "'''\n",
        "vect.fit(X)\n",
        "dtm = vect.transform(X)\n",
        "print(vect.get_feature_names())\n",
        "print(type(dtm))\n",
        "print(dtm.todense())\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clooAQkiGfBx"
      },
      "source": [
        "### Define Search Space\n",
        "Look for both the best hyperparameters of vectorizer and classification model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3R9iNggGfBx"
      },
      "source": [
        "# COMPLETE THE CODE IN THIS CELL\n",
        "# Parameters to search in dictionary \n",
        "import numpy as np\n",
        "parameters = {\n",
        "    'vect__max_df': [0.95, 1.0],\n",
        "    'vect__min_df': range(14, 24, 2),\n",
        "    'vect__max_features': range(200, 400, 10),\n",
        "    'clf__n_estimators': range(300, 460, 10),\n",
        "    'clf__max_depth': range(10, 40, 3)\n",
        "}\n",
        "\n",
        "# Implement a grid search with cross-validation\n",
        "#grid_search = GridSearchCV(pipe, param_grid=parameters, n_jobs=-1, cv=2, verbose=1)\n",
        "#grid_search.fit(X, y)\n",
        "\n",
        "# Display the best score from the grid search\n",
        "#grid_search.best_score_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn2eNx9_H67P"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "    print('ERROR: Not connected to a TPU runtime')\n",
        "else:\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    print ('TPU address is', tpu_address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxUXbpcewoYq"
      },
      "source": [
        "grid_search = RandomizedSearchCV(\n",
        "    pipe,\n",
        "    param_distributions = parameters,\n",
        "    n_jobs = -1,\n",
        "    cv = 2,\n",
        "    verbose = 1,\n",
        "    n_iter = 500\n",
        ")\n",
        "\n",
        "grid_search.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGd5eDV6PC3X"
      },
      "source": [
        "# Display the best parameters from the grid search\n",
        "'''\n",
        "0.7408866177573559\n",
        "{'vect__min_df': 10, 'vect__max_features': 2000, 'vect__max_df': 1.0, 'clf__n_estimators': 340, 'clf__max_depth': 80}\n",
        "'''\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK4r904hGfBy"
      },
      "source": [
        "### 1.5 Make a Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-H9n9Oz2ivc"
      },
      "source": [
        "test['description']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEQo7ikVGfBy"
      },
      "source": [
        "# COMPLETE THE CODE IN THIS CELL\n",
        "# Predictions on **test** sample\n",
        "pred = grid_search.predict(test['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtE6ZpNnGfBy"
      },
      "source": [
        "# COMPLETE THE CODE IN THIS CELL\n",
        "submission = pd.DataFrame({'id' : test['id'], 'category': pred})\n",
        "submission['category'] = submission['category'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrkTTtjoGfBz"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUACBAiMGfB0"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "submission_number = 0\n",
        "\n",
        "submission.to_csv(f'submission{submission_number}.csv', index=False)\n",
        "submission_number += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N-lBN63PUa9"
      },
      "source": [
        "# Download submission to local machine from this Google Colab notebook\n",
        "from google.colab import files\n",
        "files.download(f'submission{submission_number-1}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28pqE5pn1efM"
      },
      "source": [
        "### 1.6 Submit results to `kaggle` and get score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmO0Nqwh2S6"
      },
      "source": [
        "First, upload the `kaggle.json` API token file from local machine.<br>\n",
        "Do this by clicking the file icon in the left sidebar, <br>\n",
        "then clicking file icon with an up arrow inside it at the upper left, <br>\n",
        "then navigating to and selecting the `kaggle.json` file in local machine.<br>\n",
        "`kaggle.json` is usually found in a folder called `.kaggle` local machine, <br>\n",
        "\n",
        "Then: make a folder `/root/.kaggle` in this notebook,<br>\n",
        "and copy `kaggle.json` file into the `/root/.kaggle/` folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYYxG8Fkh2S7"
      },
      "source": [
        "#!mkdir /root/.kaggle/\n",
        "!mv /kaggle.json /root/.kaggle/ \n",
        "!chmod 600 /root/.kaggle/kaggle.json # to safeguard your privacy\n",
        "!ls -l /root/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rwWt0TNGfB0"
      },
      "source": [
        "## 2. Add Latent Semantic Indexing to your pipeline (Learn)\n",
        "<a id=\"p2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "PZXSsX8rGfB0"
      },
      "source": [
        "### Follow Along\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model & try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (LSI) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
        "4. Make a submission to Kaggle \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75X-i44lGfB0"
      },
      "source": [
        "### 2.1 Define Pipeline Components\n",
        "\n",
        "Nest pipelines to perform SVD on our vectorization (LSA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG_lFmOyGfB1"
      },
      "source": [
        "# COMPLETE THE CODE IN THIS CELL\n",
        "# Transforming our Vectorization with SVD is how LSA generates topic columns\n",
        "svd = TruncatedSVD(algorithm='randomized', n_iter=10)\n",
        "\n",
        "# vectorizer and classifier like before\n",
        "vect = TfidfVectorizer(stop_words=\"english\")\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# LSA pipeline with vectorizer & truncated SVD\n",
        "lsa = Pipeline([('vect', vect), ('svd', svd)])\n",
        "\n",
        "# combine LSA pipeline together with classifier\n",
        "pipe = Pipeline([('lsa', lsa), ('clf', clf)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF7_arccGfB1"
      },
      "source": [
        "### 2.2 Define Your grid search space and run a grid search with cross-validation\n",
        "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANy4UkVeGfB1"
      },
      "source": [
        "# COMPLETE THE CODE IN THIS CELL\n",
        "'''\n",
        "0.7408866177573559\n",
        "{'vect__min_df': 10, 'vect__max_features': 2000, 'vect__max_df': 1.0, 'clf__n_estimators': 340, 'clf__max_depth': 80}\n",
        "'''\n",
        "parameters = {\n",
        "    'lsa__svd__n_components': range(10, 40, 2),\n",
        "    'lsa__svd__n_iter': range(2, 17, 3),\n",
        "    'lsa__vect__max_df': [0.98, 1.0],\n",
        "    'lsa__vect__min_df': range(10, 20, 2),\n",
        "    'lsa__vect__max_features': range(200, 400, 20),\n",
        "    'clf__n_estimators': range(320, 420, 10),\n",
        "    #'clf__max_depth': range(70, 90, 2),\n",
        "}\n",
        "\n",
        "grid_search = RandomizedSearchCV(\n",
        "    pipe,\n",
        "    param_distributions = parameters,\n",
        "    n_jobs = -1,\n",
        "    cv = 2,\n",
        "    verbose = 1,\n",
        "    n_iter = 600\n",
        ")\n",
        "\n",
        "grid_search.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAXQ0qZfRNWu"
      },
      "source": [
        "grid_search.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oVwOkS9RPLY"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xZotkIEGfB2"
      },
      "source": [
        "### 2.3 Make a Submission File\n",
        "See section $1.6$ above for instructions on how to submit your results file to `kaggle` and get your score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25fSUB-7GfB2"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = grid_search.predict(test['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAawXel0GfB2"
      },
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'category':pred})\n",
        "submission['category'] = submission['category'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VxekvAdGfB2"
      },
      "source": [
        "# Make Sure the Category is an Integer\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mirtMVSWGfB2"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "submission_number = 0\n",
        "submission.to_csv(f'submission{submission_number}.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1_gHQfLTu5T"
      },
      "source": [
        "# Download submission to your local machine from this Colab notebook\n",
        "from google.colab import files\n",
        "files.download(f'submission{submission_number}.csv')\n",
        "submission_number +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLtOAlokGfB3"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Continue to apply Latent Semantic Indexing (LSI) to various datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_6YzbjHGfB3"
      },
      "source": [
        "# 3. Add Spacy Word Embeddings\n",
        "<a id=\"p3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuAIyBRPGfB4"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "What you should be doing now:\n",
        "1. Join the Kaggle Competition\n",
        "2. Download the data\n",
        "3. Train a model & try: \n",
        "    - Creating a Text Extraction & Classification Pipeline\n",
        "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
        "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
        "    - Try to extract word embeddings with Spacy and use document vectors made from those word embeddings as your features for a classification model.\n",
        "4. Make a submission to Kaggle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voankIL7GfB3"
      },
      "source": [
        "### 3.1 Process the data set with spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTE0VTyMGfB3"
      },
      "source": [
        "# Apply to your Dataset\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from scipy.stats import randint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxESV0tdGfB4"
      },
      "source": [
        "# Continue Word Embedding Work Here\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_word_vectors(docs):\n",
        "    # YOUR CODE HERE\n",
        "    return [nlp(d).vector for d in docs]\n",
        "\n",
        "X_train_emb = get_word_vectors(train['description'])\n",
        "X_test_emb = get_word_vectors(test['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFyjQuQ_6tcE"
      },
      "source": [
        "train['description'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfTZhdriCVCZ"
      },
      "source": [
        "rf = GradientBoostingClassifier()\n",
        "params = { \n",
        "    'n_estimators': range(280, 400, 10), \n",
        "    'max_depth': range(6, 20, 2)\n",
        "}\n",
        "\n",
        "rsrf = RandomizedSearchCV(rf,\n",
        "                  param_distributions=params, \n",
        "                  cv=2, \n",
        "                  n_jobs=-1, \n",
        "                  verbose=1,\n",
        "                  n_iter=20)\n",
        "rsrf.fit(X_train_emb, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sErmIzPmUcb4"
      },
      "source": [
        "# massively overfit with the Random Forest\n",
        "print('Training Accuracy: ', rsrf.score(X_train_emb, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6gj2dtAsojE"
      },
      "source": [
        "Here we use oob_score_ (out-of-bag score) as a **proxy** for the test score;<br>\n",
        "for your submission, you will predict on the test set, as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emXFzc-EUkIo"
      },
      "source": [
        "# validation looks decent without any tuning\n",
        "print(rsrf.best_score_)\n",
        "print(rsrf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNUdL4rtGfB4"
      },
      "source": [
        "### 3.2 Make a Submission File\n",
        "See section $1.6$ above for instructions on how to submit your results file to `kaggle` and get your score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqyoR6Ad05ze"
      },
      "source": [
        "### Make a Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWVF6R5Gpxw-"
      },
      "source": [
        "# Predictions on test sample\n",
        "pred = rfc.predict(X_test_emb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BhnJhOB8iRg"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "submission = pd.DataFrame({'id': test['id'], 'category':pred})\n",
        "submission['category'] = submission['category'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnrL_7Uz05zl"
      },
      "source": [
        "# Save your Submission File\n",
        "# Best to Use an Integer or Timestamp for different versions of your model\n",
        "submission_number = 2\n",
        "submission.to_csv(f'submission{submission_number}.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOuvuXk405zm"
      },
      "source": [
        "# Download submission to local machine from Google Colab\n",
        "from google.colab import files\n",
        "files.download(f'submission{submission_number}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Aebtg2iiA5L"
      },
      "source": [
        "### 3.3 Submit your predictions to Kaggle\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn8vb9F78ojn"
      },
      "source": [
        "# YOUR CODE HERE "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU7Oep--uLPR"
      },
      "source": [
        "# Post Lecture Assignment (Stretch)\n",
        "<a id=\"p4\"></a>\n",
        "\n",
        "Your primary assignment this afternoon is to achieve a minimum of 80% accuracy on the Kaggle competition. <br>\n",
        "Once you've accomplished that, do (1), and either (2) or (3): \n",
        "\n",
        "1. Research \"Sentiment Analysis\". Provide answers in markdown to the following questions: \n",
        "    - What is \"Sentiment Analysis\"? \n",
        "    - Is Document Classification different than \"Sentiment Analysis\"? Provide evidence for your response\n",
        "    - How do people create labeled sentiment data? Are those labels really sentiment?\n",
        "    - What are common applications of sentiment analysis?\n",
        "\n",
        "2. Singular Value Decomposition (SVD) is one of the most important and powerful methods in Applied Mathematics and in all of Machine Learning.  Principal Components Analysis (PCA) -- which we used in Module 2 -- is closely releated to SVD. Research SVD using the resources below. Then write a few paragraphs explaining -- in your own words -- your understanding of SVD and why it has become so important in Machine Learning. As you write, pretend that you will be presenting this summary orally as an answer to a question during a job interview.<br>\n",
        "\n",
        "* [Daniela Witten](https://www.danielawitten.com/), a Professor of Mathematical Statistics at the University of Washington, recently penned a highly amusing and informative [tweetstorm](https://twitter.com/WomenInStat/status/1285611042446413824) about SVD, well worth reading!<br>\n",
        "* [Stanford University Lecture on SVD](https://www.youtube.com/watch?v=P5mlg91as1c) <br>\n",
        "* [StatQuest Principal Components Analysis](https://www.youtube.com/watch?v=FgakZw6K1QQ)<br>\n",
        "* [Luis Serrano Principal Components Analysis](https://www.youtube.com/watch?v=g-Hb26agBFg)<br>\n",
        "\n",
        "3. Research which other models can be used for text classification -- see [Multi-Class Text Classification Model Comparison and Selection](https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568)\n",
        "  - Try a few other classical machine learning models, and compare with the gradient boosting results \n",
        "  - Neural Networks are becoming more popular for document classification. Why is that the case? \n",
        "  - If you have the time and interest, check out this [text classification documentation](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) from Google\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8xSIsjGtE84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}